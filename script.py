import feedparser
from datetime import datetime
import requests
from bs4 import BeautifulSoup

# === Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ§Ø±ÙŠØ® ===
today = datetime.now().strftime("%Y-%m-%d")
output_file = f"daily_report_{today}.md"

# === Ù…ØµØ§Ø¯Ø± RSS ===
feeds = {
    "HackerOne": "https://hackerone.com/hacktivity.rss",
    "Exploit-DB": "https://www.exploit-db.com/rss.xml",
    "PacketStorm": "https://packetstormsecurity.com/files/feed.xml"
}

# === Twitter Ø¨Ø¯ÙŠÙ„ (Scraping Ù…Ù† Nitter) ===
hashtags = {
    "#bugbountytips": "https://nitter.net/search?f=tweets&q=%23bugbountytips",
    "#bugbounty": "https://nitter.net/search?f=tweets&q=%23bugbounty",
    "#infosec": "https://nitter.net/search?f=tweets&q=%23infosec"
}

def fetch_tweets_from_nitter(url, max_tweets=5):
    try:
        response = requests.get(url, timeout=10)
        soup = BeautifulSoup(response.text, "html.parser")
        tweets = soup.find_all("div", class_="tweet-content")
        return [tweet.text.strip() for tweet in tweets[:max_tweets]]
    except Exception as e:
        return [f"_Error fetching tweets: {e}_"]

# === ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ ===
with open(output_file, "w", encoding="utf-8") as f:
    f.write(f"# ğŸ›¡ï¸ Daily Bug Bounty Digest â€” {today}\n\n")

    # Ø£Ø®Ø¨Ø§Ø± RSS
    for name, url in feeds.items():
        f.write(f"## ğŸ” {name} Updates\n")
        feed = feedparser.parse(url)

        if not feed.entries:
            f.write("_No entries found._\n\n")
            continue

        for entry in feed.entries[:5]:
            title = entry.title
            link = entry.link
            f.write(f"- [{title}]({link})\n")
        f.write("\n")

    # ØªØºØ±ÙŠØ¯Ø§Øª Ù…Ù† Ù†ÙŠØªÙ‘Ø±
    for tag, url in hashtags.items():
        f.write(f"## ğŸ¦ Tweets from {tag}\n")
        tweets = fetch_tweets_from_nitter(url)
        for tweet in tweets:
            f.write(f"- {tweet}\n")
        f.write("\n")

print(f"âœ… Report created: {output_file}")
